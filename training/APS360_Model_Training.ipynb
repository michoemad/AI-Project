{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360_Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltqUvr2g5Qu"
      },
      "source": [
        "# Data Cleaning/Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hanWw3ewg65i"
      },
      "source": [
        "import pandas as pd\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O01rc5OCnm3y"
      },
      "source": [
        "# The first time you run this will download a ~823MB file\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "                              dim=50)   # embedding size = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhAWllNI422M"
      },
      "source": [
        "# define the columns that we want to process and how to process\n",
        "# use default tokenizer (string.split())\n",
        "text_field = torchtext.legacy.data.Field(sequential=True, \n",
        "                                         include_lengths=True, \n",
        "                                         use_vocab=True,\n",
        "                                         batch_first=True)\n",
        "label_field = torchtext.legacy.data.Field(sequential=False, \n",
        "                                          use_vocab=False, \n",
        "                                          pad_token=None, \n",
        "                                          unk_token=None,\n",
        "                                          batch_first=True,\n",
        "                                          preprocessing=lambda x: int(x == 'D'))\n",
        "\n",
        "fields = [\n",
        "    ('tweet', text_field), # process it as text\n",
        "    ('id', None), # we dont need this, so no processing\n",
        "    ('conversation_id', None), # we dont need this, so no processing\n",
        "    ('party', label_field) # process it as label\n",
        "]\n",
        "\n",
        "trainds, valds, testds = torchtext.legacy.data.TabularDataset.splits(path='', \n",
        "                                                                    format='csv', \n",
        "                                                                    train='train_set.csv', \n",
        "                                                                    validation='val_set.csv',\n",
        "                                                                    test='test_set.csv', \n",
        "                                                                    fields=fields, \n",
        "                                                                    skip_header=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhOaX9LpJLr8"
      },
      "source": [
        "# Build vocab\n",
        "text_field.build_vocab(trainds)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vpEBlBh2Urb"
      },
      "source": [
        "def get_data_loader(batch_size):\n",
        "  traindl = torchtext.legacy.data.BucketIterator(trainds, # specify train and validation Tabulardataset\n",
        "                                                batch_size=batch_size,  # batch size of train and validation\n",
        "                                                sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
        "                                                sort_within_batch=True, \n",
        "                                                repeat=False)\n",
        "  \n",
        "  valdl = torchtext.legacy.data.BucketIterator(valds, # specify train and validation Tabulardataset\n",
        "                                              batch_size=batch_size,  # batch size of train and validation\n",
        "                                              sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
        "                                              sort_within_batch=True, \n",
        "                                              repeat=False)\n",
        "    \n",
        "  testdl = torchtext.legacy.data.BucketIterator(testds, # specify train and validation Tabulardataset\n",
        "                                                batch_size=batch_size,  # batch size of train and validation\n",
        "                                                sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
        "                                                sort_within_batch=True, \n",
        "                                                repeat=False)\n",
        "\n",
        "  return traindl, valdl, testdl"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eMa_Wjkya33"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7eWho6ydwe"
      },
      "source": [
        "# Example taken from lab\n",
        "\n",
        "class TweetRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(TweetRNN, self).__init__()\n",
        "        self.emb = torch.eye(input_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb[x]\n",
        "        # Set an initial hidden state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-CxTpIyzDU"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QRK9iCdyj3-"
      },
      "source": [
        "def train_network(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-5, plot=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses, valid_losses, train_acc, valid_acc = [], [], [], []\n",
        "    epochs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch.tweet[0])\n",
        "            train_loss = criterion(pred, batch.party)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        for batch in valid_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch.tweet[0])\n",
        "            valid_loss = criterion(pred, batch.party)\n",
        "            \n",
        "        train_losses.append(float(train_loss))\n",
        "        valid_losses.append(float(valid_loss))     \n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"Epoch %d; Train Loss %f; Val Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "            epoch+1, train_loss, valid_loss, train_acc[-1], valid_acc[-1]))\n",
        "\n",
        "        # TODO: Save model\n",
        "\n",
        "    # plotting\n",
        "    if plot:\n",
        "      plt.title(\"Accuracy Curve\")\n",
        "      plt.plot(epochs, train_acc, label=\"Train\")\n",
        "      plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Accuracy\")\n",
        "      plt.legend(loc='best')\n",
        "      plt.show()\n",
        "\n",
        "      plt.title(\"Loss Curve\")\n",
        "      plt.plot(epochs, train_losses, label=\"Train\")\n",
        "      plt.plot(epochs, valid_losses, label=\"Validation\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.legend(loc='best')\n",
        "      plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(valid_acc[-1]))\n",
        "\n",
        "def get_accuracy(model, data):\n",
        "    correct, total = 0, 0\n",
        "    for batch in data:\n",
        "        output = model(batch.tweet[0])\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(batch.party.view_as(pred)).sum().item()\n",
        "        total += batch.party.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9nuuDXey2Ug"
      },
      "source": [
        "input_size = len(text_field.vocab.itos)\n",
        "model = TweetRNN(input_size, 100)\n",
        "train_loader, valid_loader, test_loader = get_data_loader(256)\n",
        "train_network(model, train_loader, valid_loader, num_epochs=30, plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}